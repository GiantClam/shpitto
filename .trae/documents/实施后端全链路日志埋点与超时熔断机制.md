您遇到的问题是：前端收到 "Analyzing request" 后就没反应了。这通常意味着请求已经发送到了后端，但后端在处理 LangGraph 的第一个节点（Architect）时卡住了，或者是流式响应（Streaming）在某个环节断开了。

最可能的原因是 **OpenRouter/Gemini API 响应过慢或超时**，导致 `await architect_llm.ainvoke(...)` 这一行代码无限期挂起，而我们没有任何日志来告知当前状态。

为了精准定位并解决这个问题，我制定了以下 **“深度日志与超时熔断”** 计划：

### 1. 增强可观测性 (Add Logging)
我们将在后端的关键执行路径上埋点，打印详细的控制台日志。这样您就可以在 Terminal 中清晰地看到流程走到了哪一步。

*   **文件**: `apps/backend/agent/architect.py`
    *   **Action**: 在 `architect_node` 内部，`ainvoke` 调用前后增加 `print` 日志。
    *   **Log**: `[Architect] Invoking LLM with model: {SELECTED_MODEL}...`
    *   **Log**: `[Architect] LLM response received.`
    *   **Log**: `[Architect] Error during LLM execution: {e}`

*   **文件**: `apps/backend/api/chat.py`
    *   **Action**: 在 `format_stream` 生成器中增加日志，确认 LangGraph 事件流是否被触发。
    *   **Log**: `[Stream] Starting graph execution...`
    *   **Log**: `[Stream] Received event: {event_type}`

### 2. 防止无限挂起 (Add Timeout)
如果 LLM 服务商响应慢，我们不能让程序无限等下去。

*   **文件**: `apps/backend/agent/architect.py`
    *   **Action**: 在初始化 `ChatOpenAI` 时，显式添加 `request_timeout=120`（2分钟超时）。
    *   **Rationale**: Gemini 2.0 Flash 还是预览版，偶尔会有长尾延迟，设置超时能让我们快速捕获错误而不是干等。

### 3. 修复潜在的流式中断
有时候 `astream` 生成器如果第一个 yield 等太久，前端可能会超时断开。

*   **文件**: `apps/backend/api/chat.py`
    *   **Action**: 确保在 `Analyzing request` 之后，即使 LLM 还在思考，也通过 `asyncio.sleep` 定期发送一个“心跳”或保持连接活跃（Keep-Alive），防止前端认为连接已死。

---

### 执行步骤
1.  **修改 Architect**: 增加详细日志 + 设置 API 超时。
2.  **修改 API**: 增加流式日志。
3.  **验证**: 重启后端，再次请求，观察 Terminal 输出，精准定位是卡在“连接 LLM 之前”还是“等待 LLM 回复之中”。